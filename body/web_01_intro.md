# 数据爬虫入门

在信息化时代，互联网上的网页已经成为最丰富、最开放的数据来源之一。从商品价格、用户评论，到政府报告、科研文献，只要页面是可见的、结构是稳定的，我们往往就可以通过编程的方式批量提取这些信息。

**网页爬虫（Web Scraping）**，就是指使用程序自动访问网页，并提取其中有用信息的技术。它是数据科学、商业分析、文本挖掘等领域常见的数据获取手段之一。

在本讲中，我们将从最基础的知识讲起，带领大家一步步掌握如何用 Python 抓取静态网页上的结构化数据。重点包括：网页结构的理解、请求与解析工具的使用、分页与异常处理的实现方法等。希望通过这一讲的学习，你能够掌握编写简单爬虫程序的能力，并能将其应用于感兴趣的实际问题中。


## 爬虫工作的基本流程

一个典型的**静态网页爬虫**程序，一般包括以下几个步骤：

### 明确爬取目标

在动手之前，需要先确定三个问题：

* 
* 要抓取什么内容（如：商品名称、价格、评论数）？
* 这些内容在哪些网页上出现？
* 网页之间是否存在规律（例如分页、不同类别对应不同 URL）？

此阶段可以通过浏览器打开网页，借助「检查元素」功能 (右击网页空白处，点击 `检查`)，查看 HTML 结构，初步判断数据的分布和可抓取性。

::: {.callout-tip}
1. 你也可以右击网页空白处，点击 `查看网页源代码` (**Ctrl+U**)，进而在此页面搜索关键词查看特定类别的信息对应的网页标签。不过，这种方法不够直观，且在复杂网页中难以快速定位。
2. 在「检查」页面或「网页源代码」页面，通过搜索关键词的方式，快速定位到你想要抓取的内容所在的 HTML 标签。如果是借助 ChatGPT 等 AI 工具来辅助编写代码，可以把这些包含关键信息的网页源码截取下来，作为提示词发送给 AI 工具。这有助于提升 AI 代码的准确性和效率。
3. 也可以使用浏览器插件（如 `Web Scraper`、`Data Miner` 等）来辅助抓取网页数据。这些工具可以帮助你快速识别网页结构，并生成相应的爬虫代码。
:::


### 构造请求并获取网页内容

网页本质上是运行在远程服务器上的一段 HTML 文本。我们可以使用 Python 中的 `requests` 库模拟浏览器访问网页，通过 HTTP 请求获取网页源码。最常用的是：

* `requests.get(url)`：用于发起 GET 请求，获取静态网页内容；
* 返回的对象 `.text` 属性中包含整个 HTML 页面源码。

例如：

```python
import requests
html = requests.get("https://example.com").text
```

### 解析网页结构，提取数据

网页源代码中的数据通常嵌套在各种 HTML 标签中。我们需要使用解析库（如 `BeautifulSoup`）对 HTML 进行解析，提取我们关心的部分。例如：

* 商品名可能在 `<h2 class="title">` 标签中；
* 评论数可能在 `<span class="comment-count">` 中；
* 图片链接可能保存在 `img` 标签的 `src` 属性中。

在提取数据时，主要方法包括：

* `find()`：查找第一个匹配的元素；
* `find_all()`：查找所有符合条件的元素；
* 通过 `.text` 提取文字，或 `.get("href")`、`.get("src")` 获取链接地址。

### 保存为结构化格式

提取出的信息可以以列表或字典形式存储，最终保存为结构化文件，便于后续分析处理。常用保存格式包括：

* CSV 文件：适用于表格型数据；
* JSON 文件：适用于嵌套结构较多的复杂数据；
* 或直接用 `pandas.DataFrame` 构建表格并保存。

例如：

```python
import pandas as pd
df = pd.DataFrame(data)
df.to_csv("result.csv", index=False)
```

### 循环抓取与翻页处理

实际网页往往分多页展示数据。为了获取全部内容，需要加入分页控制逻辑，循环处理多个 URL。例如：

* 构造多个 URL（如 `page=1`、`page=2`）；
* 在 `for` 循环中遍历多个网页地址；
* 每抓一页设置 `time.sleep()` 等待几秒，防止触发反爬机制。

### 静态网页爬虫小结

整个静态网页爬虫的核心流程可以简化为：

> **确定目标网页 → 发出请求 → 获取网页内容 → 解析提取信息 → 保存结构化数据**

这五个步骤构成了网页数据抓取的基本闭环。在实际操作中，程序往往需要考虑异常处理（如网络断开、网页结构变化等），并进行适当的优化与封装。我们将在后续内容中，对每一个步骤展开详解与示范。


## 爬虫所需的 Python 基础知识

在开始编写网页爬虫之前，建议先具备一些基础的 Python 编程能力。虽然不需要非常精通，但以下几个方面的知识会在实际操作中频繁用到：

* **字符串处理**：如字符串拼接、切片、替换、查找等；
* **列表与字典**：用于存储爬取下来的数据；
* **循环与条件判断**：如 `for`、`if-else`，处理分页、异常情况等；
* **异常处理**：通过 `try...except...` 来避免程序因单页错误而中断；
* **文件读写与数据保存**：如将数据保存为 `.csv` 或 `.json` 文件；
* **正则表达式**（可选）：用于复杂的字符串匹配与提取；
* **基础的函数定义**：将爬虫逻辑封装为函数，便于复用与调试。

这些知识通常可以在《Python 编程入门》类书籍或基础教程中找到，初学者可按需回顾补充。

## 必备库简介

编写爬虫程序时，我们往往会用到几个功能明确、文档完善的第三方库。以下是爬取静态网页过程中最常用的几个工具包：

### requests

这是一个非常简洁且功能强大的 HTTP 请求库，用于向网页发出请求，获取网页的 HTML 内容。它支持 GET、POST 等多种请求方式，允许自定义请求头、携带参数，适用于绝大多数静态网页抓取任务。

* 文档链接：[https://docs.python-requests.org](https://docs.python-requests.org)

### BeautifulSoup

这是网页解析的主力工具，适用于处理 HTML 与 XML 格式的数据。通过它可以方便地查找网页中的某个标签、提取标签属性或文本内容。常用方法包括 `.find()`、`.find_all()`、`.get()` 和 `.text` 等。

* 文档链接：[https://www.crummy.com/software/BeautifulSoup/bs4/doc/](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

### 其它
- [pandas](https://pandas.pydata.org/docs/)：用于将提取到的数据组织成表格（DataFrame）格式，并导出为 `.csv`、`.xlsx` 等格式，方便后续的数据清洗与分析。

### time（内置库）

虽然不是第三方库，但在爬虫中也常用 `time.sleep()` 来设置访问间隔，模拟人类浏览行为，从而降低被封禁的风险。

### 可选推荐：lxml

对于解析速度要求更高的任务，`lxml` 是一个比 `html.parser` 更快的解析器，可与 BeautifulSoup 结合使用。安装时略复杂，但在处理大规模页面时性能更优。

* 文档链接：[https://lxml.de/](https://lxml.de/)

这些库都可以通过 `pip install` 命令快速安装，且在网上能找到大量案例与教程，适合初学者逐步探索与实践。后续内容将围绕它们展开详细介绍。
