{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89acae77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4d3a409",
   "metadata": {},
   "source": [
    "\n",
    "- [使用 PyTorch 进行自然语言处理 (NLP)](https://www.dataquest.io/blog/natural-language-processing-nlp-with-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c512db4",
   "metadata": {},
   "source": [
    "文本数据无处不在：博客、评论、聊天消息、电子邮件、支持工单、会议记录以及社交媒体帖子等等。但要大规模地理解这些数据却并非易事。与我们在数据科学中常用的整洁电子表格和数据库不同，文本数据杂乱无章、结构混乱，并且充满了人类难以理解的复杂性。\n",
    "\n",
    "*在本教程中，我们将在 PyTorch 中演示一个能够*理解文本数据的NLP 动手任务：将推文分类为真实的或虚假的灾难报告。我们将学习如何对文本进行标记化、加载预训练模型、对其进行微调，以及在不涉及任何抽象概念或高级数学的情况下评估预测结果。\n",
    "\n",
    "最后，您将在 PyTorch 中构建一个完整的 NLP 管道，可以确定推文是否描述的是真正的灾难。\n",
    "\n",
    "为什么 NLP 对数据科学家如此重要\n",
    "------------------\n",
    "\n",
    "自然语言处理支持许多常见的数据科学任务：\n",
    "\n",
    "-   **情绪分析**------从评论中理解客户意见\n",
    "-   **内容分类**------自动对支持票或新闻文章进行分类\n",
    "-   **聊天机器人和虚拟助手**------支持对话界面\n",
    "-   **信息提取**------从文本文档中提取结构化数据\n",
    "\n",
    "NLP 的独特之处在于**文本具有序列结构和上下文；**单词根据其位置以及与其他单词的关系构建含义。与表格数据不同，语言必须按序列进行解释，这需要专门的技术。\n",
    "\n",
    "### 我们的灾难推文数据集\n",
    "\n",
    "我们将使用一个真实世界的数据集：对一条推文是否描述了真实的灾难事件进行分类。该数据集非常适合学习自然语言处理 (NLP)，因为：\n",
    "\n",
    "1.  文本简短易懂\n",
    "2.  问题是二元分类（无论是否是真正的灾难）\n",
    "3.  业务相关性明确（识别实际的紧急情况）\n",
    "4.  结果很容易解释\n",
    "\n",
    "例如，考虑数据集中的这些推文：\n",
    "\n",
    "-   \"加拿大萨斯喀彻温省拉龙格附近发生森林火灾\"（*真实灾难*）\n",
    "-   \"阳光明媚，我正前往海滩#disaster #notreally\"（*不是真正的灾难*）\n",
    "\n",
    "PyTorch 中的关键 NLP 概念\n",
    "-------------------\n",
    "\n",
    "在我们进入代码之前，让我们快速回顾一下现代 NLP 系统的一些关键组件。\n",
    "\n",
    "### 标记化\n",
    "\n",
    "文本需要先转换成数字，然后神经网络才能处理它。**标记化**会将文本分解成多个部分（标记），并为每个部分分配一个唯一的数字 ID。如果您尝试过像 ChatGPT 这样的大型语言模型 (LLM)，那么您肯定遇到过标记化------这些模型在内部使用标记来理解和生成文本。\n",
    "\n",
    "现代标记器不仅仅按空格进行拆分；它们使用可以处理的子词标记方法：\n",
    "\n",
    "-   **通过将生僻词**分解成更小的片段来查找\n",
    "-   利用已知子词块拼写**错误**\n",
    "-   通过组合熟悉的子词来生成**新词或未知词**\n",
    "\n",
    "例如，\"预处理\"一词可能会被分解成诸如`\"pre\"`、`\"process\"`和 之类的词法单元`\"ing\"`，每个单元都有各自的数字词法单元 ID。像 GPT-4 这样的大型语言模型使用类似的技术，将输入文本分解成词法单元，以帮助模型高效处理海量词汇。\n",
    "\n",
    "### 嵌入和向量化\n",
    "\n",
    "一旦我们有了词条 ID，我们就需要一种能够捕捉其含义的表示方法。**嵌入**是词条的密集向量表示，它将相似的单词在高维空间中紧密排列在一起。\n",
    "\n",
    "为了理解嵌入，想象一个多维空间，其中每个单词都有其独特的位置。在这个嵌入空间中：\n",
    "\n",
    "-   含义相近的单词出现在一起\n",
    "-   意思相反的词相距很远\n",
    "-   单词之间的关系被保留为方向\n",
    "\n",
    "例如，在训练良好的**嵌入空间**中：\n",
    "\n",
    "-   \"国王\" - \"男人\" + \"王后\" ≈ \"女人\"（捕捉性别关系）\n",
    "-   \"巴黎\" - \"法国\" + \"罗马\" ≈ \"意大利\"（捕捉首都与国家的关系）\n",
    "\n",
    "像\"灾难\"这样的词可能被表示为一个由 768 个浮点数组成的向量，而类似概念，例如\"灾祸\"，则在这个嵌入空间中有一个与之相邻的向量。像\"龙卷风\"、\"地震\"和\"洪水\"这样的词会聚集在附近的区域，而像\"阳光\"或\"生日\"这样不相关的词则会聚在一起。\n",
    "\n",
    "![文本矢量化和嵌入](https://www.dataquest.io/wp-content/uploads/2025/04/Text_vectorization_and_embeddings.png.webp)\n",
    "\n",
    "[这些丰富的数字表示使得模型能够比简单的独热编码](https://www.dataquest.io/blog/kaggle-getting-started/)更好地理解语义关系，在简单的独热编码中，每个单词都与其他单词有相同的差异。\n",
    "\n",
    "### NLP 中的 Transformer\n",
    "\n",
    "一旦将 token 转换为有意义的嵌入，下一步就是使用能够有效利用这些嵌入的架构。Transformer**是**支持现代 NLP 和大型语言模型 (LLM)（包括 GPT 模型）的主流架构。与循环神经网络等较旧的架构不同，Transformer 使用一种称为**自注意力**机制（GPT 中的\"T\"代表\"Transformer\"）来处理文本，这使得它们能够：\n",
    "\n",
    "-   同时分析所有单词，大大加快训练和推理速度\n",
    "-   捕捉单词之间的复杂关系，无论它们在句子中的距离如何\n",
    "-   更有效地理解微妙的背景和细微差别\n",
    "\n",
    "在本教程中，我们将使用一个名为**DistilBERT**的转换器模型，它是广泛使用的 BERT 模型的紧凑、高效版本，以更少的参数保留了出色的性能。\n",
    "\n",
    "### NLP中的迁移学习\n",
    "\n",
    "基于 Transformer 的模型的一大优势在于它们与**迁移学习**兼容，而迁移学习是现代 NLP 的核心技术。迁移学习是指采用已在海量文本数据集上进行预训练的模型（GPT 中的\"P\"代表\"预训练\"），并针对特定的 NLP 任务（例如情绪分析、问答或灾难推文分类）对其进行微调。\n",
    "\n",
    "这种方法非常有效，因为它：\n",
    "\n",
    "-   减少对大型特定任务数据集的需求\n",
    "-   大幅缩短训练时间\n",
    "-   提高性能，尤其是在较小的数据集上\n",
    "\n",
    "例如，ChatGPT（GPT-3.5 或 GPT-4 的微调版本）利用迁移学习将通用语言模型应用于特定的对话任务。同样，在本教程中，我们将利用迁移学习和 DistilBERT 进行结合，微调其预训练权重，以将推文分类为与灾难相关或非灾难相关，从而显著简化训练过程。\n",
    "\n",
    "### 预训练模型和 Hugging Face Hub\n",
    "\n",
    "[Hugging Face](https://huggingface.co/)已成为 NLP 模型的核心枢纽，为数千个预训练模型提供统一的 API。他们的`transformers`库只需几行代码即可轻松加载、微调和部署最先进的模型。\n",
    "\n",
    "我们将使用的模型包括：\n",
    "\n",
    "-   经过预训练的 DistilBERT 库，能够理解语言模式\n",
    "-   在顶部添加一个分类头，我们将针对特定任务进行训练（微调）\n",
    "\n",
    "好的，解决了这些问题后，让我们开始构建我们的灾难推文分类器！\n",
    "\n",
    "准备数据集\n",
    "-----\n",
    "\n",
    "首先加载必要的库和数据集。如果您使用[Google Colab](https://www.dataquest.io/blog/getting-started-with-google-colab-for-deep-learning/)进行编程，则所需的库已安装。如果您在本地运行，则可能需要先安装它们：\n",
    "\n",
    "```\n",
    "# Uncomment and run if needed\n",
    "# !pip install pandas numpy matplotlib seaborn torch scikit-learn\n",
    "```\n",
    "\n",
    "现在，让我们导入必要的库并加载我们的[训练](https://dsserver-prod-resources-1.s3.amazonaws.com/nlp/train.csv)和[测试](https://dsserver-prod-resources-1.s3.amazonaws.com/nlp/test.csv)数据集。\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "# For reproducibility across both CPU and GPU\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# Additional seeds for CUDA operations\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)  # for multi-GPU setups\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "# Set Pandas display options\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "# Take a first look at the data\n",
    "print(f\"Training set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape:     {test_df.shape}\")\n",
    "train_df.head()\n",
    "```\n",
    "\n",
    "预期输出：\n",
    "\n",
    "```\n",
    "Training set shape: (7613, 5)\n",
    "Test set shape:     (3263, 4)\n",
    "   id keyword location                                               text  target\n",
    "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
    "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada       1\n",
    "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...       1\n",
    "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...       1\n",
    "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...       1\n",
    "```\n",
    "\n",
    "让我们看看我们有哪些列以及它们包含什么：\n",
    "\n",
    "```\n",
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(train_df.isnull().sum())\n",
    "# Target distribution\n",
    "print(\"\\nTarget distribution:\")\n",
    "print(train_df['target'].value_counts())\n",
    "```\n",
    "\n",
    "预期输出：\n",
    "\n",
    "```\n",
    "Missing values per column:\n",
    "id            0\n",
    "keyword      61\n",
    "location   2533\n",
    "text          0\n",
    "target        0\n",
    "dtype: int64\n",
    "Target distribution:\n",
    "0    4342\n",
    "1    3271\n",
    "Name: target, dtype: int64\n",
    "```\n",
    "\n",
    "我们可以看到：\n",
    "\n",
    "-   我们的数据集包含 7,613 条带有标记目标的推文\n",
    "-   目标是二进制的（0 = 不是灾难，1 = 真正的灾难）\n",
    "-   非灾难推文（4,342 条）比灾难推文（3,271 条）多\n",
    "-   许多推文缺失`location`，有些缺少`keyword`信息\n",
    "    -   由于我们只对`text`和感兴趣`target`，所以我们不需要担心处理丢失的数据\n",
    "\n",
    "### 用词云探索数据\n",
    "\n",
    "让我们使用词云来可视化文本数据，以便更好地理解灾难和非灾难推文的内容。如果您在本地运行此代码，则可能需要先安装以下必需的库才能继续：\n",
    "\n",
    "```\n",
    "# Uncomment and run if needed\n",
    "# !pip install nltk wordcloud\n",
    "```\n",
    "\n",
    "现在，让我们为每个类别创建词云：\n",
    "\n",
    "```\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "# Download stopwords if needed\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def create_wordcloud(text_series, title):\n",
    "    # Combine all text\n",
    "    text = ' '.join(text_series)\n",
    "    # Create and generate a word cloud image\n",
    "    wordcloud = WordCloud(width=800, height=400,\n",
    "                          background_color='white',\n",
    "                          stopwords=stop_words,\n",
    "                          max_words=150,\n",
    "                          collocations=False).generate(text)\n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "# Create word clouds for each class\n",
    "create_wordcloud(train_df[train_df['target'] == 1]['text'],\n",
    "                 'Words in Disaster Tweets')\n",
    "print()\n",
    "create_wordcloud(train_df[train_df['target'] == 0]['text'],\n",
    "                 'Words in Non-Disaster Tweets')\n",
    "```\n",
    "\n",
    "![灾难推文词云](https://www.dataquest.io/wp-content/uploads/2025/04/Disaster_tweet_WordCloud.jpg.webp)\n",
    "\n",
    "![非灾难推文词云](https://www.dataquest.io/wp-content/uploads/2025/04/Non_disaster_tweet_WordCloud.jpg.webp)\n",
    "\n",
    "观察这些词云，我们可以注意到几个有趣的模式：\n",
    "\n",
    "1.  **灾难推文**（上图）突出地出现了\"灾难\"、\"火灾\"、\"警察\"、\"死亡\"、\"洪水\"、\"紧急情况\"和\"炸弹\"等词语------这些词语显然与灾难性事件有关。\n",
    "2.  **非灾难推文**（下图）包含更多日常用语，如\"喜欢\"、\"得到\"、\"新的\"、\"日子\"、\"时间\"、\"去\"和\"人们\"。\n",
    "3.  **编码问题**：两个词云都显示\"Û\"字符，这是由于特殊字符未正确解码而发生的编码错误。这表明我们需要在预处理过程中处理字符编码问题。\n",
    "4.  **URL 和 Twitter 特定内容**：两个云都显示\"http\"和\"co\"（来自\" <http://t.co/> ...\"链接），这表明我们需要在预处理期间删除 URL。\n",
    "5.  **RT 和 amp**：我们发现 Twitter 特有的术语，如\"RT\"（转发）和\"amp\"（来自 HTML 编码的\"&\"符号），对于分类没有任何价值。\n",
    "\n",
    "这些见解将为我们的文本预处理策略提供参考。\n",
    "\n",
    "### 基本文本预处理\n",
    "\n",
    "根据我们的词云分析，我们需要在将文本数据输入模型之前对其进行清理。尽管现代 Transformer 非常稳健，但我们仍应解决以下几个问题：\n",
    "\n",
    "```\n",
    "def clean_text(text):\n",
    "    \"\"\"Basic text cleaning function\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    # Remove Twitter-specific content\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#', '', text)     # Remove hashtag symbols but keep the words\n",
    "    text = re.sub(r'rt\\s+', '', text)  # Remove RT (retweet) indicators\n",
    "    # Remove HTML entities (like &)\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Handle encoding errors like 'Û'\n",
    "    text = re.sub(r'Û', '', text)\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "# Apply cleaning to the text column\n",
    "train_df['cleaned_text'] = train_df['text'].apply(clean_text)\n",
    "test_df['cleaned_text'] = test_df['text'].apply(clean_text)\n",
    "# Display a few examples of cleaned text\n",
    "print(\"Original vs Cleaned:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {train_df['text'].iloc[i]}\")\n",
    "    print(f\"Cleaned: {train_df['cleaned_text'].iloc[i]}\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "预期输出：\n",
    "\n",
    "```\n",
    "Original vs Cleaned:\n",
    "Original: Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
    "Cleaned: our deeds are the reason of this earthquake may allah forgive us all\n",
    "Original: Forest fire near La Ronge Sask. Canada\n",
    "Cleaned: forest fire near la ronge sask canada\n",
    "Original: All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
    "Cleaned: all residents asked to shelter in place are being notified by officers no other evacuation or shelter in place orders are expected\n",
    "```\n",
    "\n",
    "这种预处理解决了我们在词云中发现的问题：\n",
    "\n",
    "1.  删除 URL（包括\"http\"和\"t.co\"链接）\n",
    "2.  消除 Twitter 特定内容，例如提及和 RT 指标\n",
    "3.  处理诸如\"Û\"字符之类的编码问题\n",
    "4.  删除特殊字符、数字和多余的空格\n",
    "5.  将所有文本转换为小写以保持一致性\n",
    "\n",
    "将文本转换为张量\n",
    "--------\n",
    "\n",
    "现在，我们将文本数据转换为 PyTorch 可以处理的格式。为此，我们将使用 Hugging Face`transformers`库，它已成为 NLP 中使用 Transformer 模型的标准工具包。\n",
    "\n",
    "如果您在本地运行此代码，则需要安装`transformers`库：\n",
    "\n",
    "```\n",
    "# Uncomment and run if needed\n",
    "# !pip install transformers\n",
    "```\n",
    "\n",
    "现在让我们设置我们的标记器：\n",
    "\n",
    "```\n",
    "from transformers import DistilBertTokenizer\n",
    "# Load pretrained tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Example of tokenization\n",
    "example_text = \"PyTorch is great for NLP\"\n",
    "tokens = tokenizer.tokenize(example_text)\n",
    "token_ids = tokenizer.encode(example_text)\n",
    "print(f\"Original text: {example_text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n",
    "```\n",
    "\n",
    "预期输出：\n",
    "\n",
    "```\n",
    "Original text: PyTorch is great for NLP\n",
    "Tokens: ['p', '##yt', '##or', '##ch', 'is', 'great', 'for', 'nl', '##p']\n",
    "Token IDs: [101, 1052, 22123, 2953, 2818, 2003, 2307, 2005, 17953, 2361, 102]\n",
    "```\n",
    "\n",
    "让我们创建一个函数来使用上面的方法标记我们的文本数据`DistilBertTokenizer`：\n",
    "\n",
    "```\n",
    "def tokenize_text(texts, tokenizer, max_length=128):\n",
    "    \"\"\"\n",
    "    Tokenize a list of texts using the provided tokenizer\n",
    "    Returns input IDs and attention masks\n",
    "    \"\"\"\n",
    "    # Tokenize all texts at once\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encodings['input_ids'], encodings['attention_mask']\n",
    "```\n",
    "\n",
    "### 理解注意力面具\n",
    "\n",
    "当我们使用上述函数对文本进行标记时，我们得到两个重要的输出：\n",
    "\n",
    "1.  **输入 ID**：这些是我们的标记的数字表示。每个单词或子单词都会根据标记器词汇表转换为唯一的数字。\n",
    "2.  **注意力掩码**：这些是二进制张量（仅包含 0 和 1），它们告诉模型哪些标记需要\"注意\"以及哪些标记需要忽略。\n",
    "\n",
    "为什么我们*需要*注意力掩码？因为我们是批量处理推文的，而推文的长度各不相同。为了使所有序列的长度保持一致，我们会在较短的序列中添加**填充标记**。注意力掩码中，真实标记用 1 表示，填充标记用 0 表示，这告诉模型：\"关注*真实内容*，忽略填充。\"\n",
    "\n",
    "例如，如果我们有一条推文，其长度为 10 个标记，但我们要填充到 128 个标记，则注意力掩码将如下所示：\n",
    "\n",
    "```\n",
    "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ..., 0]\n",
    "```\n",
    "\n",
    "这种机制对于 Transformer 模型至关重要，因为它可以确保它们不会尝试从人工填充标记中提取意义。\n",
    "\n",
    "创建数据集和数据加载器\n",
    "-----------\n",
    "\n",
    "接下来，我们需要将数据分成训练集和验证集，并准备进行训练。这涉及几个步骤：\n",
    "\n",
    "1.  **拆分数据**：我们将数据集分为训练数据（模型学习的数据）和验证数据（我们用来评估模型性能的数据）\n",
    "2.  **对文本进行标记**：我们将所有文本转换为标记 ID 和注意掩码\n",
    "3.  **创建 DataLoaders**：我们设置高效的管道，在训练期间将数据提供给我们的模型\n",
    "\n",
    "这一过程值得我们关注，因为它：\n",
    "\n",
    "-   通过提供单独的评估数据帮助防止过度拟合\n",
    "-   支持批处理，加快训练速度\n",
    "-   标准化模型的输入形状\n",
    "\n",
    "让我们实现以下步骤：\n",
    "\n",
    "```\n",
    "# Split data into train and validation sets\n",
    "train_texts, val_texts, train_targets, val_targets = train_test_split(\n",
    "    train_df['cleaned_text'].values,\n",
    "    train_df['target'].values,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=train_df['target']  # Maintain class distribution\n",
    ")\n",
    "print(f\"Training texts: {len(train_texts)}\")\n",
    "print(f\"Validation texts: {len(val_texts)}\")\n",
    "# Set the batch size for effecient training\n",
    "batch_size = 16\n",
    "# Process training data\n",
    "train_input_ids, train_attention_masks = tokenize_text(train_texts, tokenizer)\n",
    "train_targets = torch.tensor(train_targets, dtype=torch.long)\n",
    "# Process validation data\n",
    "val_input_ids, val_attention_masks = tokenize_text(val_texts, tokenizer)\n",
    "val_targets = torch.tensor(val_targets, dtype=torch.long)\n",
    "# Create tensor datasets\n",
    "train_dataset = TensorDataset(\n",
    "    train_input_ids,\n",
    "    train_attention_masks,\n",
    "    train_targets\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    val_input_ids,\n",
    "    val_attention_masks,\n",
    "    val_targets\n",
    ")\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "# Look at a single batch\n",
    "batch = next(iter(train_loader))\n",
    "input_ids, attention_mask, targets = batch\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "```\n",
    "\n",
    "预期输出：\n",
    "\n",
    "```\n",
    "Training texts: 6851\n",
    "Validation texts: 762\n",
    "Input IDs shape: torch.Size([16, 128])\n",
    "Attention mask shape: torch.Size([16, 128])\n",
    "Targets shape: torch.Size([16])\n",
    "```\n",
    "\n",
    "这个输出告诉我们：\n",
    "\n",
    "-   我们有 6,851 条推文用于训练，762 条推文用于验证\n",
    "-   每批包含 16 条推文（我们选择的批次大小）\n",
    "-   每条推文由 128 个标记（包括填充）表示\n",
    "-   注意力掩码与输入形状相匹配，1 表示真实标记，0 表示填充\n",
    "-   我们的目标只是每条推文 0 或 1（非灾难或灾难）\n",
    "\n",
    "微调预训练的 Transformer\n",
    "------------------\n",
    "\n",
    "现在，我们准备加载一个预训练的 Transformer 模型，并对其进行微调，以完成灾难推文分类任务。选择 Transformer 模型时，应考虑模型大小、推理速度、任务复杂度以及可用的计算资源等因素。对于推文分类（一项相对简单的短文本任务）而言，像 DistilBERT 这样规模较小、效率更高的 Transformer 是理想之选，因为它在速度和准确率之间取得了平衡，且无需耗费大量资源。\n",
    "\n",
    "让我们首先建立模型：\n",
    "\n",
    "```\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Load pretrained model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2  # Binary classification\n",
    ")\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "```\n",
    "\n",
    "预期输出：\n",
    "\n",
    "```\n",
    "Using device: cuda\n",
    "model.safetensors: 100%\n",
    " 268M/268M [00:01<00:00, 257MB/s]\n",
    "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
    "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "```\n",
    "\n",
    "这个输出告诉我们一些重要的事情：\n",
    "\n",
    "1.  我们正在使用 GPU 进行训练（感谢 Google Colab！）\n",
    "2.  模型加载成功（268MB参数）\n",
    "3.  一些模型权重是新初始化的------特别是我们在基础模型之上添加的分类层\n",
    "4.  该模型需要进行训练才能用于预测（这正是我们要做的！）\n",
    "\n",
    "### 了解 F1 分数\n",
    "\n",
    "在开始训练之前，让我们先了解如何评估我们的模型。我们将使用**F1 分数**，它是分类任务中常用的指标，尤其是在类别不平衡的情况下。\n",
    "\n",
    "F1 分数是精确度和召回率的调和平均值：\n",
    "\n",
    "-   **准确度**：在我们预测为灾难的所有推文中，有多少条实际上是灾难？\n",
    "-   **回想一下**：在所有实际的灾难推文中，我们正确识别了多少条？\n",
    "\n",
    "F1 = 2 *（精确度*/召回率）/（精确度+召回率）\n",
    "\n",
    "我们使用 F1 而不是准确度，因为：\n",
    "\n",
    "1.  它平衡了准确率和召回率，这对于灾害检测很重要\n",
    "2.  它适用于不平衡数据集（其中一个类比另一个类出现得更多）\n",
    "3.  它对假阳性和假阴性都会进行惩罚\n",
    "\n",
    "### 训练模型\n",
    "\n",
    "现在，让我们实现训练循环。训练过程在 GPU 上通常需要 3-5 分钟，在 CPU 上则需要几个小时：\n",
    "\n",
    "```\n",
    "# Training function\n",
    "def train_epoch(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    for batch in data_loader:\n",
    "        # Unpack and move batch to device\n",
    "        input_ids, attention_mask, targets = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=targets\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # Calculate accuracy\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        total_predictions += len(targets)\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total_predictions\n",
    "    return avg_loss, accuracy\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Unpack and move batch to device\n",
    "            input_ids, attention_mask, targets = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=targets\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            total_loss += loss.item()\n",
    "            # Calculate accuracy\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            total_predictions += len(targets)\n",
    "            # Store targets and predictions for F1 score\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    # Calculate average loss and accuracy\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_predictions.double() / total_predictions\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    return avg_loss, accuracy, f1\n",
    "# Training loop\n",
    "epochs = 3\n",
    "best_f1 = 0\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "    # Evaluate\n",
    "    val_loss, val_acc, val_f1 = evaluate(model, val_loader, device)\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        # In a real scenario, we'd save the model here\n",
    "        # torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    print()\n",
    "```\n",
    "\n",
    "预期输出：\n",
    "\n",
    "```\n",
    "Epoch 1/3\n",
    "Train Loss: 0.4351, Train Accuracy: 0.8097\n",
    "Val Loss: 0.3737, Val Accuracy: 0.8491, Val F1: 0.8130\n",
    "Epoch 2/3\n",
    "Train Loss: 0.3300, Train Accuracy: 0.8678\n",
    "Val Loss: 0.3899, Val Accuracy: 0.8399, Val F1: 0.8045\n",
    "Epoch 3/3\n",
    "Train Loss: 0.2410, Train Accuracy: 0.9085\n",
    "Val Loss: 0.4127, Val Accuracy: 0.8451, Val F1: 0.8072\n",
    "```\n",
    "\n",
    "查看这些结果，我们可以看到：\n",
    "\n",
    "1.  **训练损失和准确率**：正如预期，随着模型的学习，训练指标稳步提升。到第 3 个 epoch 时，该模型在训练数据上的准确率已接近 91%。\n",
    "2.  **验证性能**：验证指标反映的情况更为复杂。虽然准确率保持相对稳定（约为 84-85%），但我们发现 F1 分数有所波动，最佳值通常出现在第一个 epoch 之后。\n",
    "3.  **潜在的过拟合**：训练集和验证集指标（尤其是损失）之间的差距越来越大，表明模型开始对训练数据过拟合。验证集 F1 在 epoch 1 之后略有下降，这表明我们可能需要在生产场景中考虑应用一些[正则化。](https://www.dataquest.io/blog/regularization-in-machine-learning/)\n",
    "\n",
    "评估结果\n",
    "----\n",
    "\n",
    "现在我们已经训练了我们的模型，让我们在验证集上更彻底地评估它，以了解它的优势和局限性。\n",
    "\n",
    "```\n",
    "# Detailed evaluation\n",
    "model.eval()\n",
    "all_targets = []\n",
    "all_preds = []\n",
    "all_probs = []  # For prediction probabilities\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        # Unpack and move batch to device\n",
    "        input_ids, attention_mask, targets = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # Get predictions\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        # Store targets, predictions, and probabilities\n",
    "        all_targets.extend(targets.cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())  # For positive class\n",
    "# Classification report\n",
    "print(classification_report(all_targets, all_preds, target_names=['Not Disaster', 'Disaster']))\n",
    "# Confusion matrix\n",
    "cm = pd.crosstab(\n",
    "    pd.Series(all_targets, name='Actual'),\n",
    "    pd.Series(all_preds, name='Predicted')\n",
    ")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "预期输出：\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "Not Disaster       0.83      0.91      0.87       435\n",
    "    Disaster       0.87      0.76      0.81       327\n",
    "    accuracy                           0.85       762\n",
    "   macro avg       0.85      0.83      0.84       762\n",
    "weighted avg       0.85      0.85      0.84       762\n",
    "```\n",
    "\n",
    "分类报告和混淆矩阵提供了有价值的见解：\n",
    "\n",
    "1.  **总体准确度**：我们的模型在验证集上达到了 85% 的准确度，这对于只需极少预处理的复杂 NLP 任务来说相当不错。\n",
    "2.  **类别表现**：该模型在\"非灾难\"推文（F1 得分为 0.87）上的表现略好于\"灾难\"推文（F1 得分为 0.81）。\n",
    "\n",
    "![混淆矩阵](https://www.dataquest.io/wp-content/uploads/2025/04/Confusion_matrix.png.webp)\n",
    "\n",
    "1.  **混淆矩阵分析**：查看混淆矩阵，我们可以看到：\n",
    "    -   397 条真阴性（正确识别的非灾难推文）\n",
    "    -   247 条真实阳性（正确识别的灾难推文）\n",
    "    -   38 条误报（非灾难推文被错误地标记为灾难）\n",
    "    -   80 条假阴性（错过灾难推文）\n",
    "2.  **错误类型影响**：该模型漏报真实灾害（80 个误报）的可能性略高于发出误报（38 个误报）。在现实世界的灾害监测系统中，可能需要根据漏报真实紧急情况与调查误报的相对成本来调整这种权衡。\n",
    "\n",
    "### 根据新数据进行预测\n",
    "\n",
    "最后，让我们使用我们的模型对测试集进行预测：\n",
    "\n",
    "```\n",
    "# Process test data\n",
    "test_input_ids, test_attention_masks = tokenize_text(test_df['cleaned_text'].values, tokenizer)\n",
    "# Create test dataloader\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Unpack and move batch to device\n",
    "        input_ids, attention_mask = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        # Get predictions\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        # Store predictions and probabilities\n",
    "        test_preds.extend(preds.cpu().numpy())\n",
    "        test_probs.extend(probs[:, 1].cpu().numpy())\n",
    "# Add predictions to test dataframe\n",
    "test_df['predicted_target'] = test_preds\n",
    "test_df['disaster_probability'] = test_probs\n",
    "# Display a sample of predictions\n",
    "print(\"Sample predictions on the test set:\")\n",
    "sample_results = test_df[['text', 'predicted_target', 'disaster_probability']].sample(10)\n",
    "sample_results\n",
    "```\n",
    "\n",
    "预期输出：\n",
    "\n",
    "```\n",
    "Sample predictions on the test set:\n",
    "                                                                                                    text    predicted_target    disaster_probability\n",
    "149  #MustRead: Vladimir #Putin Issues Major Warning But Is It Too Late To Escape Armageddon? by @PCr...                   1                0.820633\n",
    "2028    tarmineta3: Breaking news! Unconfirmed! I just heard a loud bang nearby. in what appears to be a...                0                0.079320\n",
    "2559                                      @MeganRestivo I am literally screaming for you!! Congratulations!                0                0.011177\n",
    "800                                                          @PahandaBear @Nethaera Yup EU crashed too :P                  0                0.039989\n",
    "1237    Angry Woman Openly Accuses NEMA Of Stealing Relief Materials Meant For IDPs: An angry Internally...                0                0.078757\n",
    "2448    Photo: theonion: Rescuers Heroically Help Beached Garbage Back Into OceanåÊ http://t.co/YcSmt7ovoc                 1                0.739298\n",
    "1566    ...@jeremycorbyn must be willing to fight and 2 call a spade a spade. Other wise very savvy piec...                0                0.032905\n",
    "691              Emergency services called to Bacup after 'strong' chemical smells http://t.co/hJJ7EFTJ7O                1                0.934740\n",
    "3103    T Shirts $10 male or female get wit me. 2 days until its game changin time. War Zone single will...                0                0.015829\n",
    "2325    #Np love police @PhilCollinsFeed On #LateNiteMix Uganda Broadcasting Corporation. UBC 98FM #Radi...                0                0.340198\n",
    "\n",
    "```\n",
    "\n",
    "结果\n",
    "--\n",
    "\n",
    "观察上面的测试预测，我们可以看到，该模型**对带有明显紧急相关语言的推文**（例如涉及化学气味或紧急警告的推文）分配较高的灾难概率。相反，它对**明显非灾难性的推文**（包括个人庆祝活动、随意对话和促销内容）分配的灾难概率非常低。\n",
    "\n",
    "一个有趣的案例是*《洋葱报》*上一条关于救援人员和垃圾的讽刺推文。虽然模型将其预测为灾难（0.74），但这突显了它**受到与灾难相关的文字词汇的强烈影响**，即使在幽默语境中使用也是如此。另一方面，一条提到*\"警察\"*一词的推文得分中等（约0.34），但仍然被正确分类为非灾难。这表明模型已经学会了某些关键词既可以出现在灾难场景中，也可以出现在非灾难场景中。\n",
    "\n",
    "总体而言，这些预测反映了一种学会平衡文字线索和更广泛背景的模型，即使没有大量的预处理或长期的训练也能表现出稳定的性能。\n",
    "\n",
    "后续步骤和资源\n",
    "-------\n",
    "\n",
    "准备好进一步提升你的 NLP 技能了吗？以下是一些自然而然的后续步骤：\n",
    "\n",
    "### 尝试其他任务\n",
    "\n",
    "-   **多类分类**：尝试按主题对新闻文章进行分类\n",
    "-   **情绪分析**：将电影评论分为正面或负面\n",
    "-   **命名实体识别**：从文本中提取人物、地点和组织\n",
    "\n",
    "### 探索更多高级技术\n",
    "\n",
    "-   尝试其他 Transformer 模型（BERT、RoBERTa、T5）\n",
    "-   文本数据增强实验\n",
    "-   实施注意力可视化来解释模型决策\n",
    "-   添加早期停止以防止过度拟合（正如我们在训练结果中看到的那样）\n",
    "-   应用交叉验证进行更稳健的评估\n",
    "-   探索超参数调整以优化学习率、批量大小等。\n",
    "\n",
    "### 推荐资源\n",
    "\n",
    "-   [拥抱脸部文档](https://huggingface.co/docs)\n",
    "-   [PyTorch NLP 教程](https://pytorch.org/tutorials/)\n",
    "-   [PyTorch 入门](https://www.dataquest.io/blog/pytorch-for-deep-learning/)\n",
    "-   [PyTorch 中的序列模型](https://www.dataquest.io/blog/sequence-models-in-pytorch/)\n",
    "\n",
    "文本数据可能看起来杂乱复杂，但像 PyTorch 和预训练的 Transformer 这样的现代工具却让它变得出奇地容易上手。从小处着手，循序渐进，往往是提升 NLP 技能和信心的最佳途径。\n",
    "\n",
    "关键术语回顾\n",
    "------\n",
    "\n",
    "-   **标记化**：将文本转换为标记（单词、子单词或字符）的过程\n",
    "-   **嵌入**：捕捉语义关系的标记的密集向量表示\n",
    "-   **迁移学习**：将预训练模型中的知识应用于新任务\n",
    "-   **Transformer**：一种利用自注意力机制处理序列的神经网络架构\n",
    "-   **微调**：通过更新参数使预训练模型适应特定任务\n",
    "-   **注意力机制**：一种允许模型关注输入相关部分的机制\n",
    "-   **注意掩码**：二进制张量，告诉模型哪些标记是真实内容，哪些是填充\n",
    "-   **F1 分数**：平衡精度和召回率的指标，适用于不平衡数据集\n",
    "-   **Hugging Face**：为 NLP 提供工具和预训练模型的组织"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
